<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://nds-shlab.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://nds-shlab.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-08-13T09:58:06+00:00</updated><id>https://nds-shlab.github.io/feed.xml</id><title type="html">blank</title><subtitle>NDS Group @ Shanghai AI Lab </subtitle><entry><title type="html">由Ring-Attention性能问题引发的计算通信overlap分析</title><link href="https://nds-shlab.github.io/blog/2024/ring/" rel="alternate" type="text/html" title="由Ring-Attention性能问题引发的计算通信overlap分析"/><published>2024-07-03T12:00:00+00:00</published><updated>2024-07-03T12:00:00+00:00</updated><id>https://nds-shlab.github.io/blog/2024/ring</id><content type="html" xml:base="https://nds-shlab.github.io/blog/2024/ring/"><![CDATA[<h2 id="abstract">Abstract</h2> <p>我们在长文本训练场景下尝试接入Ring-Flash-Attention时，由于通信与计算无法充分重叠，不能达到预期的优化收益。其根源来自于GPU上计算和通信内核的调度顺序与kernel lanch顺序不完全一致。经过调研发现，NeMo通过设置CUDA_DEVICE_MAX_CONNECTIONS环境变量来控制启动顺序从而实现预期的overlap，但这种方式存在单一stream上虚假依赖的副作用。我们探索了通过引入event和占位计算的方式实现了同样的效果。在实践中，可以根据场景选择合适的方式。</p>]]></content><author><name></name></author><category term="LLMs"/><category term="Training"/><category term="System"/><summary type="html"><![CDATA[Abstract]]></summary></entry><entry><title type="html">大模型的高效训练：从Infra到框架优化</title><link href="https://nds-shlab.github.io/blog/2024/survey/" rel="alternate" type="text/html" title="大模型的高效训练：从Infra到框架优化"/><published>2024-07-03T12:00:00+00:00</published><updated>2024-07-03T12:00:00+00:00</updated><id>https://nds-shlab.github.io/blog/2024/survey</id><content type="html" xml:base="https://nds-shlab.github.io/blog/2024/survey/"><![CDATA[<h2 id="abstract">Abstract</h2> <p>我们最近发表了一篇关于大规模语言模型（LLMs）高效训练的综述，这项工作历经半年时间，汇聚了实验室内外多个小组成员的支持和贡献，即使在他们手头项目十分紧迫的情况下，依旧投入了大量时间和精力。此外，我们还要特别感谢NTU和北大的老师们的倾力协助。https://arxiv.org/abs/2407.20018</p>]]></content><author><name></name></author><category term="LLMs"/><category term="Training"/><category term="System"/><summary type="html"><![CDATA[Abstract]]></summary></entry><entry><title type="html">由A800平台训练InternLM-7B无法收敛引发的思考</title><link href="https://nds-shlab.github.io/blog/2024/a800/" rel="alternate" type="text/html" title="由A800平台训练InternLM-7B无法收敛引发的思考"/><published>2024-06-29T12:00:00+00:00</published><updated>2024-06-29T12:00:00+00:00</updated><id>https://nds-shlab.github.io/blog/2024/a800</id><content type="html" xml:base="https://nds-shlab.github.io/blog/2024/a800/"><![CDATA[<h2 id="abstract">Abstract</h2> <p>大模型训练（如InternLM-7B）实践中，曾经遇到过在A100集群上表现正常的代码和数据，迁移到A800集群却出现了模型准确度下降和梯度范数爆炸的问题。经过调查，我们发现这与A800和A100 GPU的NVLink带宽差异有关。通过在两个集群上使用nanoGPT模型进行的对照实验，我们确认了精度差异的原因在于NCCL的Ring all-reduce算法实现。进一步实验表明，设置环境变量NCCL_ALGO=Tree或使用gloo作为backend可以解决精度对齐问题。最终，我们提出了一个解决方案：在A800集群上设置NCCL_ALGO=Tree，强制使用Tree算法进行all-reduce操作，从而避免了Ring算法带来的精度问题，使得A800集群的模型能够正常收敛，并且与A100集群的训练精度对齐。</p>]]></content><author><name></name></author><category term="LLMs"/><category term="Training"/><category term="System"/><summary type="html"><![CDATA[Abstract]]></summary></entry><entry><title type="html">Understanding the Workload Characteristics of Large Language Model Development</title><link href="https://nds-shlab.github.io/blog/2024/acme/" rel="alternate" type="text/html" title="Understanding the Workload Characteristics of Large Language Model Development"/><published>2024-04-19T12:00:00+00:00</published><updated>2024-04-19T12:00:00+00:00</updated><id>https://nds-shlab.github.io/blog/2024/acme</id><content type="html" xml:base="https://nds-shlab.github.io/blog/2024/acme/"><![CDATA[<h2 id="abstract">Abstract</h2> <p>Large Language Models (LLMs) have presented impressive performance across several transformative tasks, such as chatbot and code generation. However, it is non-trivial to efficiently utilize large-scale cluster resources to develop LLMs, often riddled with numerous challenges such as frequent hardware failures, intricate parallelization strategies, and imbalanced resource utilization. A thorough analysis of cluster workloads is essential for comprehending challenges and uncovering opportunities in designing systems tailored for LLMs.</p> <p>To this end, we present an in-depth characterization study of a six-month LLM development workload trace collected from our GPU datacenter Acme of Shanghai AI Laboratory. Specifically, we investigate discrepancies between LLMs and prior task-specific Deep Learning (DL) workloads, explore resource utilization patterns, and identify the impact of various job failures.</p>]]></content><author><name></name></author><category term="LLMs"/><category term="Training"/><category term="System"/><summary type="html"><![CDATA[Abstract]]></summary></entry></feed>