<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://nds-shlab.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://nds-shlab.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-06-29T12:06:52+00:00</updated><id>https://nds-shlab.github.io/feed.xml</id><title type="html">blank</title><subtitle>NDS Group @ Shanghai AI Lab </subtitle><entry><title type="html">由A800平台训练InternLM-7B无法收敛引发的思考</title><link href="https://nds-shlab.github.io/blog/2024/a800/" rel="alternate" type="text/html" title="由A800平台训练InternLM-7B无法收敛引发的思考"/><published>2024-06-29T12:00:00+00:00</published><updated>2024-06-29T12:00:00+00:00</updated><id>https://nds-shlab.github.io/blog/2024/a800</id><content type="html" xml:base="https://nds-shlab.github.io/blog/2024/a800/"><![CDATA[<h2 id="abstract">Abstract</h2> <p>大模型训练（如InternLM-7B）实践中，曾经遇到过在A100集群上表现正常的代码和数据，迁移到A800集群却出现了模型准确度下降和梯度范数爆炸的问题。经过调查，我们发现这与A800和A100 GPU的NVLink带宽差异有关。通过在两个集群上使用nanoGPT模型进行的对照实验，我们确认了精度差异的原因在于NCCL的Ring all-reduce算法实现。进一步实验表明，设置环境变量NCCL_ALGO=Tree或使用gloo作为backend可以解决精度对齐问题。最终，我们提出了一个解决方案：在A800集群上设置NCCL_ALGO=Tree，强制使用Tree算法进行all-reduce操作，从而避免了Ring算法带来的精度问题，使得A800集群的模型能够正常收敛，并且与A100集群的训练精度对齐。</p>]]></content><author><name></name></author><category term="LLMs"/><category term="formatting"/><category term="links"/><summary type="html"><![CDATA[Abstract]]></summary></entry><entry><title type="html">Understanding the Workload Characteristics of Large Language Model Development</title><link href="https://nds-shlab.github.io/blog/2024/acme/" rel="alternate" type="text/html" title="Understanding the Workload Characteristics of Large Language Model Development"/><published>2024-04-19T12:00:00+00:00</published><updated>2024-04-19T12:00:00+00:00</updated><id>https://nds-shlab.github.io/blog/2024/acme</id><content type="html" xml:base="https://nds-shlab.github.io/blog/2024/acme/"><![CDATA[<h2 id="abstract">Abstract</h2> <p>Large Language Models (LLMs) have presented impressive performance across several transformative tasks, such as chatbot and code generation. However, it is non-trivial to efficiently utilize large-scale cluster resources to develop LLMs, often riddled with numerous challenges such as frequent hardware failures, intricate parallelization strategies, and imbalanced resource utilization. A thorough analysis of cluster workloads is essential for comprehending challenges and uncovering opportunities in designing systems tailored for LLMs.</p> <p>To this end, we present an in-depth characterization study of a six-month LLM development workload trace collected from our GPU datacenter Acme of Shanghai AI Laboratory. Specifically, we investigate discrepancies between LLMs and prior task-specific Deep Learning (DL) workloads, explore resource utilization patterns, and identify the impact of various job failures.</p>]]></content><author><name></name></author><category term="LLMs"/><category term="formatting"/><category term="links"/><summary type="html"><![CDATA[Abstract]]></summary></entry></feed>